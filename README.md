# PromptBreeder
Google DeepMind's PromptBreeder (https://arxiv.org/pdf/2309.16797.pdf) for automated prompt engineering implemented using LangChain expression language(LCEL).

## Install

## Usage

## Notes

- Why does it say each EvolutionUnit has 2 task-prompts? When are those two generated? Do you evolve both the same each time? If so do you just call the LLM twice? Use the best for fitness I assume...

- When it comes to context, should I be saving the "workings-out" generated by the fitness function for each prompt? Looks like it. todo.

## In Progress

    [ ] - bug fixes
    [ ] - switch to batched LLM calls
    [ ] - jupyter lab demo
    [ ] - Implement prompt_crossover
    [ ] - Implement context_shuffling
    [ ] - Implement estimation_distribution_mutation

## Complete

    [ ] - mutator operations
    [x] - GSM8K dataset implementation
    [x] - LangChain BaseLLM for model (plug and play)

## Future

    [ ] - different datasets
    [ ] - better logging
    [ ] - LLM as a fitness function
    [ ] - improve fitness scoring