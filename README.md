# PromptBreeder
Google DeepMind's PromptBreeder (https://arxiv.org/pdf/2309.16797.pdf) for automated prompt engineering implemented using LangChain expression language(LCEL).

**Currently only supports VertexAI.** It's built to accept any LangChain BaseLLM, so swapping is possible, just not supported currently. 

## Setup

`git clone https://github.com/vaughanlove/PromptBreeder`

`cd PromptBreeder`

`python -m venv venv `

`venv/Scripts/activate // windows`

`source venv/bin/activate // unix`

`pip install -r requirements.txt`

    Make sure you have installed gcloud cli (https://cloud.google.com/sdk/docs/install) and have authenticated with your Google Cloud Platform (GCP) project. Your GCP project needs to have vertex ai enabled. 

## Usage

``` python .\main.py -mp 2 -ts 4 -e 10 -n 40 -problem "Solve the math word problem, giving your answer as an arabic numeral." ```

Args:

- `mp` - number of mutation prompts.
- `ts` - number of thinking styles.
- `e` - number of fitness examples to evaluate over.
- `n` - number of generations to simulate.
- `problem` - the problem_description.

note: the number of units is determined by `# units` = `mp` * `ts`.

## Outputs

The two best prompts for the 40th gen:

P1: 

    Give me the correct examples of his workings out. \n\nAdvice: When multiplying a number by a decimal, you can move the decimal point in the multiplier to the right by the same number of places as there are decimal places in the multiplicand. For example, 5 x $1.50 can be rewritten as 5 x $1.50 = 5 x 150/100 = $7.50 \n\nWhen adding or subtracting decimals, you need to line up the decimal points. For example, $7.50 + $7.50 = $15.0

P0:

     planted with corn and soybeans from the total number of acres to find the number of acres planted with wheat.** \n\n- 100acres - 40 acres - 30 acres = 30 acres of wheat\n\n**Final Answer:** The farmer plants 30 acres of wheat.

The entire output can be found in [example_output](example_output.txt).

## In Progress

    [x] - switch to batched LLM calls - check out the mt branch.
    [ ] - Implement prompt_crossover
    [ ] - Implement context_shuffling
    [ ] - Implement estimation_distribution_mutation

## Complete

    [ ] - mutator operations
    [x] - GSM8K dataset implementation
    [x] - LangChain BaseLLM for model (plug and play)

## Future

    [ ] - different datasets
    [x] - better logging
    [ ] - LLM as a fitness function
    [ ] - improve fitness scoring

## Notes

- Why does it say each EvolutionUnit has 2 task-prompts? When are those two generated? Do you evolve both the same each time? If so do you just call the LLM twice? Use the best for fitness I assume...

- When it comes to context, should I be saving the "workings-out" generated by the fitness function for each prompt? Looks like it. todo.

- In zero_order_prompt_gen, I changed it from "a list of 100 hints: " to "an ordered list of 100 hints: " to make regexing the first hint easier.

## Ideas

Potentially useful for generating prompts that are more 'humanlike' - useful for beating perplexity/burstiness checks. What you could do is define a fitness function to measure perplexity.

## Issues

 - Slow, tried to multithread but doesn't actually increase the speed of the program since Google rate limits to the speed of their batch() function to 60 inference calls/minute anyways.

 - Since it is slow, I only check fitness on batches of 4 by default. This can be scaled up, but will slow down the program significantly.

 - The way I assess if a question is "correct" is just parsing the LLM output for the presence of the answer. This could lead to False Positives.
